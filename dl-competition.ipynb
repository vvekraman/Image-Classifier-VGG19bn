{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport csv\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check GPU availability\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"transform = transforms.Compose([transforms.ToTensor(),\n                                transforms.RandomApply(torch.nn.ModuleList([transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5)]), p=0.5),\n                                transforms.RandomRotation(10),\n                                transforms.RandomGrayscale(p=0.1),\n                                transforms.RandomHorizontalFlip(),\n                                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n                                #transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5)\n                               ])\n                                #transforms.Grayscale(num_output_channels=3)\n                                #torchvision.transforms.Grayscale(num_output_channels=3)\n                                #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n                               \n\ntransform_pred = transforms.Compose([transforms.ToTensor(), \n                                     transforms.Resize((224,224)),\n                                     #torchvision.transforms.Grayscale(num_output_channels=3)\n                                     #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                                     #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n                                    ])\n\ndata_set = torchvision.datasets.ImageFolder(root='../input/dldatasetresized/train_resized', transform=transform)\npred_data = torchvision.datasets.ImageFolder(root='../input/dl-pred-data', transform=transform_pred)\n\ntrain_set, test_set =torch.utils.data.random_split(data_set, (2608, 1118))  # 2980, 746\n\ntrainloader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2) \ntestloader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)\npredloader = torch.utils.data.DataLoader(pred_data, batch_size=1, shuffle=False, num_workers=2)\n\nclasses = ('christmas_cookies', 'christmas_presents', 'christmas_tree', 'fireworks',\n           'penguin', 'reindeer', 'santa', 'snowman')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# functions to show an image\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n# print labels\nprint(' '.join('%5s' % classes[labels[j]] for j in range(64)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"vgg16 = models.vgg19_bn(pretrained=True)\n\n\n# change the number of classes \nvgg16.classifier[6].out_features = 8\n\n# freeze convolution weights\nfor param in vgg16.features.parameters():\n    param.requires_grad = False\n\"\"\"\"\"\nvgg16.classifier[0].out_features = 10096\nvgg16.classifier[3].in_features = 10096\nvgg16.classifier[3].out_features = 4096\nvgg16.classifier[6].in_features = 4096\nvgg16.classifier[6].out_features = 8\n\n\nvgg16.classifier.add_module(\"7\", torch.nn.ReLU(inplace=True))\nvgg16.classifier.add_module(\"8\", torch.nn.Dropout(p=0.5, inplace=False))\nvgg16.classifier.add_module(\"9\", torch.nn.Linear(in_features=4096, out_features=1096, bias=True))\nvgg16.classifier.add_module(\"10\", torch.nn.ReLU(inplace=True))\nvgg16.classifier.add_module(\"11\", torch.nn.Dropout(p=0.5, inplace=False))\nvgg16.classifier.add_module(\"12\", torch.nn.Linear(in_features=1096, out_features=8, bias=True))  \n\"\"\"\nvgg16.to(device)\nprint(vgg16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"vgg16 = models.densenet201(pretrained=True)\n#vgg16 = models.vgg19_bn(pretrained=True)\nvgg16.to(device)\n\n# change the number of classes \nvgg16.classifier.out_features = 8\n\n# freeze convolution weights\nfor param in vgg16.features.parameters():\n    param.requires_grad = False\n    \n#print(vgg16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer\noptimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.01, momentum=0.9)\n#optimizer = torch.optim.Adam(vgg16.classifier.parameters(), lr=0.001, amsgrad=False)\n\n# loss function\ncriterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# optimizer\n#optimizer = optim.SGD(vgg16.classifier.parameters(), lr=0.005, momentum=0.9)\n#optimizer = torch.optim.Adam(vgg16.classifier.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n\n# loss function\n#criterion = nn.CrossEntropyLoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# training function\ndef fit(model, train_dataloader, epoch, n_epochs):\n\n    model.train()\n\n    train_running_loss = 0.0\n    train_running_correct = 0\n\n    print(f'Epoch {epoch+1}/{n_epochs}')\n    #pbar = tf.keras.utils.Progbar(target=len(train_dataloader))\n\n    for i, data in enumerate(train_dataloader):\n        data, target = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()  # set the gradients to zero before starting to do backpropragation\n        output = model(data)\n        loss = criterion(output, target)\n        train_running_loss += loss.item()\n        _, preds = torch.max(output.data, 1) # takes the highest val in an array and removes the rest, also returns the index of the highest value\n        train_running_correct += (preds == target).sum().item()\n        loss.backward() # computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x\n        optimizer.step() # updates the value of x using the gradient x.grad\n        #pbar.update(i, values=[(\"loss\",train_running_loss/len(train_dataloader.dataset))])\n\n    train_loss = train_running_loss/len(train_dataloader.dataset)\n    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n    \n    return train_loss, train_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# validation/test function\ndef validate(model, test_dataloader):\n    \n    model.eval() # Dropout layer inactive\n    \n    val_running_loss = 0.0\n    val_running_correct = 0\n    \n    for i, data in enumerate(test_dataloader):\n        data, target = data[0].to(device), data[1].to(device)\n        output = model(data)\n        loss = criterion(output, target)  \n        val_running_loss += loss.item()\n        _, preds = torch.max(output.data, 1)\n        val_running_correct += (preds == target).sum().item()\n    \n    val_loss = val_running_loss/len(test_dataloader.dataset)\n    val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n    print('Validation Loss: ',val_loss , '  Validation Acc: ', val_accuracy)\n    \n    return val_loss, val_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# epoch\ntrain_loss , train_accuracy = [], []\nval_loss , val_accuracy = [], []\n\nstart = time.time()\nn_epochs = 25\n\nfor epoch in range(n_epochs):\n    train_epoch_loss, train_epoch_accuracy = fit(vgg16, trainloader, epoch, n_epochs)\n    val_epoch_loss, val_epoch_accuracy = validate(vgg16, testloader)\n    \n    # save the whole model\n    torch.save(vgg16, 'entire_model_ep' + str(epoch+1) + '.pt')\n    \n    train_loss.append(train_epoch_loss)\n    #train_accuracy.append(train_epoch_accuracy)\n    \n    val_loss.append(val_epoch_loss)\n    #val_accuracy.append(val_epoch_accuracy)\n\nend = time.time()\n\nprint((end-start)/60, 'minutes')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train/val accuracy plot\nplt.figure(figsize=(10, 7))\n\nplt.plot(train_accuracy, color='green', label='train accuracy')\nplt.plot(val_accuracy, color='blue', label='validataion accuracy')\nplt.legend()\n\nplt.savefig('accuracy.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train/val loss plot\nplt.figure(figsize=(10, 7))\n\nplt.plot(train_loss, color='orange', label='train loss')\nplt.plot(val_loss, color='red', label='validataion loss')\nplt.legend()\n\nplt.savefig('loss.png')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# load model\nmodel = torch.load('entire_model_ep9.pt')\nmodel.to(device)\n#print(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the state_dict()\nPATH = 'entire_model_ep2_statedict().pth'\ntorch.save(model.state_dict(), PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"model = models.vgg16_bn(pretrained=True)\nmodel.load_state_dict(torch.load(PATH))\n\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction fn\ndef prediction(model, predloader):\n    \n    model.eval()\n    \n    result = {}\n    \n    for count, data in enumerate(predloader):\n        data = data[0].to(device)\n        output = model(data)\n        _,preds = torch.max(output.data, 1)\n        sample_fname, _ = predloader.dataset.samples[count]\n        result[sample_fname[26:-4]] = preds[0].item() \n    return result\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# prediction\na = prediction(model, predloader)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('predictions.csv', 'w') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"Id\", \"Category\"])\n    writer.writeheader()\n    [f.write('{0},{1}\\n'.format(key, value)) for key, value in a.items()]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}